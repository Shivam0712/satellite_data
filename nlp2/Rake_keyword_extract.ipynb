{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nlp_rake import rake\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim.models import word2vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stoppath = 'new_stoplist.txt'\n",
    "#stoppath = 'SmartStoplist.txt'\n",
    "#stoppath = 'FoxStoplist.txt'\n",
    "rake_object = rake.Rake(stoppath, 5, 3, 4)\n",
    "\n",
    "\n",
    "def onlySentences(article):\n",
    "    new_article = \"\"\n",
    "    for sentence in sent_tokenize(article):\n",
    "        if dist in sentence.lower():\n",
    "            new_article += sentence \n",
    "            new_article += '\\n'\n",
    "    return new_article\n",
    "\n",
    "def pureText(article):\n",
    "    and_idx1 = article.find(\"|&| \")\n",
    "    and_idx2 = article[and_idx1+4:].find(\"|&| \") + and_idx1 + 4\n",
    "    and_idx3 = article.rfind(\"|&| \")\n",
    "\n",
    "    return article[and_idx3+4:]\n",
    "\n",
    "def articleDate(article):\n",
    "    try:\n",
    "        and_idx1 = article.find(\"|&| \")\n",
    "        and_idx2 = article[and_idx1+4:].find(\"|&| \") + and_idx1 + 4\n",
    "        and_idx3 = article.rfind(\"|&| \")\n",
    "\n",
    "        article_date = article[and_idx2+4:and_idx3]\n",
    "\n",
    "        dash_idx1 = article_date.find('-')\n",
    "\n",
    "        dash_idx2 = article_date.rfind('-')\n",
    "\n",
    "        year = int(article_date[:dash_idx1])\n",
    "        month = int(article_date[dash_idx1+1:dash_idx2])\n",
    "        day = int(article_date[dash_idx2+1:])\n",
    "\n",
    "        return [article_date,year,month,day]\n",
    "\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "def do_lmtz(article):\n",
    "    lmtz = WordNetLemmatizer()\n",
    "    words = word_tokenize(article)\n",
    "    new_article = \"\"\n",
    "    for word in words:\n",
    "        new_word = lmtz.lemmatize(word,'v')\n",
    "        new_article = new_article + \" \" + new_word\n",
    "    return new_article\n",
    "\n",
    "def finalSet(article_list,start_idx,end_idx):\n",
    "    article = ''\n",
    "    for i in range(start_idx,end_idx):\n",
    "        temp_article = article_list[i]\n",
    "\n",
    "        text = pureText(temp_article)\n",
    "        sents = onlySentences(text)\n",
    "        lmtzd_sent = do_lmtz(sents)\n",
    "        lmtzd_text = do_lmtz(text)\n",
    "\n",
    "        article += (text)\n",
    "    return article\n",
    "\n",
    "def extractKeywords(article_list,start_idx,min_articles,number_keywords_want):\n",
    "    articles_taken = min_articles\n",
    "\n",
    "    end_idx = start_idx+articles_taken\n",
    "    end_idx = min(end_idx,len(article_list)-1)\n",
    "\n",
    "    article = finalSet(article_list,start_idx,end_idx)\n",
    "\n",
    "    while(len(rake_object.run(article)) < number_keywords_want) and end_idx < len(article_list)-1:\n",
    "        articles_taken += 1\n",
    "        end_idx = start_idx+articles_taken\n",
    "\n",
    "        article = finalSet(article_list,start_idx,end_idx)\n",
    "\n",
    "    start_idx += articles_taken\n",
    "    keywords = rake_object.run(article)\n",
    "    print len(keywords)\n",
    "    print\n",
    "    keyword_0 = []\n",
    "    for keyword in keywords:\n",
    "        keyword_0.append(keyword[0])\n",
    "        print keyword\n",
    "\n",
    "    return [keywords,keyword_0]\n",
    "\n",
    "def extractKeywords2(article_list,start_idx,end_idx):\n",
    "    article = finalSet(article_list,start_idx,end_idx)\n",
    "    keywords = rake_object.run(article)\n",
    "    keyword_0 = []\n",
    "    for keyword in keywords:\n",
    "        keyword_0.append(keyword[0])\n",
    "    return [keywords,keyword_0,article]\n",
    "\n",
    "\n",
    "def articleList():\n",
    "    date_ = str(year) + '-'\n",
    "    if month<10:\n",
    "        date_ += '0'+str(month)\n",
    "    else:\n",
    "        date_ += str(month)\n",
    "\n",
    "    folder_name = dist+'/txt_files/'\n",
    "    f_name = folder_name+dist + '_' + date_ + '.txt'\n",
    "    f = open(f_name,'r')\n",
    "    articles = f.read()\n",
    "\n",
    "    article_list = articles.split('\\n\\n')\n",
    "    return article_list\n",
    "\n",
    "def countDays(article_list):\n",
    "    curr_idx = 0\n",
    "    count_dayAr = []\n",
    "    for day in range(1,32):\n",
    "        itr_day = day\n",
    "        count_day = 0\n",
    "        start_idx = curr_idx\n",
    "        while(itr_day<day+1):\n",
    "            curr_idx += 1\n",
    "            if curr_idx>=len(article_list):\n",
    "                break;\n",
    "            article = article_list[curr_idx]\n",
    "            if len(article) > 0:\n",
    "                count_day += 1\n",
    "                itr_day =  articleDate(article)[3]\n",
    "        count_dayAr.append([day,count_day,start_idx,curr_idx])\n",
    "    return count_dayAr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist = 'thiruvallur'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year = 2015\n",
    "month = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article_list = articleList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[keywords,keyword_0,article] = extractKeywords2(article_list,0,len(article_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 0, 1],\n",
       " [2, 7, 1, 8],\n",
       " [3, 13, 8, 21],\n",
       " [4, 5, 21, 26],\n",
       " [5, 9, 26, 35],\n",
       " [6, 14, 35, 49],\n",
       " [7, 8, 49, 57],\n",
       " [8, 19, 57, 76],\n",
       " [9, 8, 76, 84],\n",
       " [10, 6, 84, 90],\n",
       " [11, 10, 90, 100],\n",
       " [12, 8, 100, 108],\n",
       " [13, 17, 108, 125],\n",
       " [14, 16, 125, 141],\n",
       " [15, 11, 141, 152],\n",
       " [16, 16, 152, 168],\n",
       " [17, 6, 168, 174],\n",
       " [18, 6, 174, 180],\n",
       " [19, 6, 180, 186],\n",
       " [20, 17, 186, 203],\n",
       " [21, 10, 203, 213],\n",
       " [22, 5, 213, 218],\n",
       " [23, 11, 218, 229],\n",
       " [24, 13, 229, 242],\n",
       " [25, 10, 242, 252],\n",
       " [26, 15, 252, 267],\n",
       " [27, 19, 267, 286],\n",
       " [28, 7, 286, 293],\n",
       " [29, 11, 293, 304],\n",
       " [30, 11, 304, 315],\n",
       " [31, 10, 315, 327]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countDays(article_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
